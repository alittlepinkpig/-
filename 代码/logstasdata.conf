input{
#设置采集数据流的类型（file）
file{
#设置监控的文件
             path=>["/data/Log/*.csv"]
#设 置启动Logstash之后采集的初始位置（beginning，每次运行logstash时都重头开始采集）
              start_position=>"beginning"
#设置扫描的时间间隔，和写入的时间间隔(单位为s)
              discover_interval=>60
               sincedb_write_interval=>60
               }
}
#数据流处理
filter{
            csv{
                   columns=>["sales_data","selling_price","bedrooms_num","bathroom_num","housing_area","parking_area","floor_num"
,"housing_rating","built_area","basement_area","year_built","year_repair","latitude","longitude"]
                convert=>{"sales_data"=>"date" "selling_price"=>"integer" "bedrooms_num"=>"float" "bathroom_num"=>"float" "housing_area"=>"integer" 
"parking_area"=>"integer" "floor_num"=>"float" "housing_rating"=>"integer" "built_area"=>"integer" "basement_area"=>"integer"
"year_built"=>"integer" "year_repair"=>"integer" "latitude"=>"float" "longitude"=>"float"}
                          remove_field=>["message"]
                }
         mutate{
                          gsub=>["longitude","/r",""]
                }
         mutate{
                          add_field=>["[geoip][location]","%{longitude}"]
                          add_field=>["[geoip][location]","%{latitude}"]
                }
         mutate{
                          convert=>["[geoip][location]","float"]
                }
}
output{
             kafka{
                      bootstrap_servers=>"master2:9092,slave1:9092,slave2:9092"
                      topic_id=>"house"
                     codec=>"json"
                     acks=>"1"
           }
}



 